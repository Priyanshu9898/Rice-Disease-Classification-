{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d7eaf7ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from keras import layers, models\n",
    "from  keras.preprocessing.image import ImageDataGenerator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6e02735c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "train_datagen = ImageDataGenerator(\n",
    "    rescale = 1./255,\n",
    "    rotation_range = 40, \n",
    "    width_shift_range = 0.2, \n",
    "    height_shift_range = 0.2, \n",
    "    shear_range = 0.2,\n",
    "    zoom_range = 0.2, \n",
    "    horizontal_flip = True)\n",
    "\n",
    "val_datagen = ImageDataGenerator( \n",
    "    rescale = 1.0/255 \n",
    "\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e5c0e943",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 2402 images belonging to 3 classes.\n",
      "Found 268 images belonging to 3 classes.\n"
     ]
    }
   ],
   "source": [
    "train_generator = train_datagen.flow_from_directory(\n",
    "    \"Dataset/Original/train/\", \n",
    "    batch_size = 32, \n",
    "    class_mode = 'sparse', \n",
    "    target_size = (224, 224))\n",
    "\n",
    "validation_generator = val_datagen.flow_from_directory( \n",
    "    \n",
    "    \"Dataset/Original/val/\",\n",
    "    batch_size = 32, \n",
    "    class_mode = 'sparse', \n",
    "    target_size = (224, 224)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a62a26be",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from tensorflow.keras.applications.vgg16 import VGG16\n",
    "\n",
    "model = VGG16(input_shape = (224, 224, 3),\n",
    "                   \n",
    "    include_top = False, \n",
    "                   \n",
    "    weights = 'imagenet'\n",
    "\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6e0518a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "for layer in model.layers:\n",
    "    layer.trainable = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7baaa00d",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = layers.Flatten()(model.output)\n",
    "\n",
    "\n",
    "x = layers.Dense(256, activation='relu')(x)\n",
    "\n",
    "# Add a dropout rate of 0.5\n",
    "# x = layers.Dropout(0.5)(x)\n",
    "\n",
    "\n",
    "x = layers.Dense(3 , activation='softmax')(x)\n",
    "\n",
    "model = tf.keras.models.Model(model.input, x)\n",
    "\n",
    "model.compile(\n",
    "    optimizer='adam',\n",
    "    loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=False),\n",
    "    metrics=['accuracy']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "00c8e50e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "76/76 [==============================] - 24s 261ms/step - loss: 1.3279 - accuracy: 0.6074 - val_loss: 0.6360 - val_accuracy: 0.7500\n",
      "Epoch 2/30\n",
      "76/76 [==============================] - 18s 241ms/step - loss: 0.7129 - accuracy: 0.7057 - val_loss: 0.5419 - val_accuracy: 0.7873\n",
      "Epoch 3/30\n",
      "76/76 [==============================] - 19s 249ms/step - loss: 0.6383 - accuracy: 0.7394 - val_loss: 0.5249 - val_accuracy: 0.8060\n",
      "Epoch 4/30\n",
      "76/76 [==============================] - 19s 244ms/step - loss: 0.6260 - accuracy: 0.7460 - val_loss: 0.5043 - val_accuracy: 0.7948\n",
      "Epoch 5/30\n",
      "76/76 [==============================] - 19s 248ms/step - loss: 0.6368 - accuracy: 0.7327 - val_loss: 0.5204 - val_accuracy: 0.7836\n",
      "Epoch 6/30\n",
      "76/76 [==============================] - 19s 245ms/step - loss: 0.5584 - accuracy: 0.7835 - val_loss: 0.5350 - val_accuracy: 0.8022\n",
      "Epoch 7/30\n",
      "76/76 [==============================] - 19s 248ms/step - loss: 0.5620 - accuracy: 0.7681 - val_loss: 0.5583 - val_accuracy: 0.7761\n",
      "Epoch 8/30\n",
      "76/76 [==============================] - 19s 244ms/step - loss: 0.5715 - accuracy: 0.7735 - val_loss: 0.5844 - val_accuracy: 0.7612\n",
      "Epoch 9/30\n",
      "76/76 [==============================] - 19s 248ms/step - loss: 0.5316 - accuracy: 0.7823 - val_loss: 0.4953 - val_accuracy: 0.7985\n",
      "Epoch 10/30\n",
      "76/76 [==============================] - 19s 243ms/step - loss: 0.5350 - accuracy: 0.7885 - val_loss: 0.5569 - val_accuracy: 0.7873\n",
      "Epoch 11/30\n",
      "76/76 [==============================] - 19s 246ms/step - loss: 0.5208 - accuracy: 0.7873 - val_loss: 0.4422 - val_accuracy: 0.8284\n",
      "Epoch 12/30\n",
      "76/76 [==============================] - 19s 242ms/step - loss: 0.5175 - accuracy: 0.7902 - val_loss: 0.4590 - val_accuracy: 0.8209\n",
      "Epoch 13/30\n",
      "76/76 [==============================] - 19s 251ms/step - loss: 0.5262 - accuracy: 0.7956 - val_loss: 0.4397 - val_accuracy: 0.8507\n",
      "Epoch 14/30\n",
      "76/76 [==============================] - 19s 246ms/step - loss: 0.5068 - accuracy: 0.7973 - val_loss: 0.5182 - val_accuracy: 0.7910\n",
      "Epoch 15/30\n",
      "76/76 [==============================] - 19s 250ms/step - loss: 0.4819 - accuracy: 0.8006 - val_loss: 0.4642 - val_accuracy: 0.8246\n",
      "Epoch 16/30\n",
      "76/76 [==============================] - 19s 248ms/step - loss: 0.5229 - accuracy: 0.7898 - val_loss: 0.4171 - val_accuracy: 0.8619\n",
      "Epoch 17/30\n",
      "76/76 [==============================] - 19s 246ms/step - loss: 0.4902 - accuracy: 0.8018 - val_loss: 0.4677 - val_accuracy: 0.8172\n",
      "Epoch 18/30\n",
      "76/76 [==============================] - 19s 245ms/step - loss: 0.4769 - accuracy: 0.8068 - val_loss: 0.5804 - val_accuracy: 0.7799\n",
      "Epoch 19/30\n",
      "76/76 [==============================] - 19s 247ms/step - loss: 0.4649 - accuracy: 0.8143 - val_loss: 0.4755 - val_accuracy: 0.8396\n",
      "Epoch 20/30\n",
      "76/76 [==============================] - 19s 245ms/step - loss: 0.4688 - accuracy: 0.8093 - val_loss: 0.4616 - val_accuracy: 0.8321\n",
      "Epoch 21/30\n",
      "76/76 [==============================] - 19s 246ms/step - loss: 0.4417 - accuracy: 0.8264 - val_loss: 0.4709 - val_accuracy: 0.8284\n",
      "Epoch 22/30\n",
      "76/76 [==============================] - 19s 242ms/step - loss: 0.4590 - accuracy: 0.8168 - val_loss: 0.4962 - val_accuracy: 0.8321\n",
      "Epoch 23/30\n",
      "76/76 [==============================] - 19s 246ms/step - loss: 0.4191 - accuracy: 0.8410 - val_loss: 0.4356 - val_accuracy: 0.8321\n",
      "Epoch 24/30\n",
      "76/76 [==============================] - 19s 244ms/step - loss: 0.4270 - accuracy: 0.8389 - val_loss: 0.4082 - val_accuracy: 0.8433\n",
      "Epoch 25/30\n",
      "76/76 [==============================] - 19s 249ms/step - loss: 0.4296 - accuracy: 0.8372 - val_loss: 0.4858 - val_accuracy: 0.8134\n",
      "Epoch 26/30\n",
      "76/76 [==============================] - 19s 245ms/step - loss: 0.4211 - accuracy: 0.8218 - val_loss: 0.4306 - val_accuracy: 0.8470\n",
      "Epoch 27/30\n",
      "76/76 [==============================] - 19s 250ms/step - loss: 0.4216 - accuracy: 0.8276 - val_loss: 0.4310 - val_accuracy: 0.8694\n",
      "Epoch 28/30\n",
      "76/76 [==============================] - 19s 246ms/step - loss: 0.4471 - accuracy: 0.8114 - val_loss: 0.4406 - val_accuracy: 0.8545\n",
      "Epoch 29/30\n",
      "76/76 [==============================] - 19s 251ms/step - loss: 0.4571 - accuracy: 0.8231 - val_loss: 0.4477 - val_accuracy: 0.8396\n",
      "Epoch 30/30\n",
      "76/76 [==============================] - 19s 249ms/step - loss: 0.4390 - accuracy: 0.8289 - val_loss: 0.4132 - val_accuracy: 0.8657\n"
     ]
    }
   ],
   "source": [
    "vgghist = model.fit(\n",
    "    train_generator, \n",
    "    validation_data = validation_generator,\n",
    "    steps_per_epoch = 76, \n",
    "    epochs = 30\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "82222917",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "75.0625"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "2402 / 32"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5252a1ad",
   "metadata": {},
   "source": [
    "## InceptionV3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f431f20e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 2402 images belonging to 3 classes.\n",
      "Found 268 images belonging to 3 classes.\n"
     ]
    }
   ],
   "source": [
    "train_generator_inception = train_datagen.flow_from_directory(\n",
    "    \"Dataset/Original/train/\", \n",
    "    batch_size = 32, \n",
    "    class_mode = 'sparse', \n",
    "    target_size = (150, 150))\n",
    "\n",
    "validation_generator_inception = val_datagen.flow_from_directory( \n",
    "    \n",
    "    \"Dataset/Original/val/\",\n",
    "    batch_size = 32, \n",
    "    class_mode = 'sparse', \n",
    "    target_size = (150, 150)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00dcd876",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "16b36789",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.applications.inception_v3 import InceptionV3\n",
    "base_model = InceptionV3(input_shape = (150, 150, 3), include_top = False, weights = 'imagenet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "5bc86153",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "for layer in base_model.layers:\n",
    "    layer.trainable = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f5cfc748",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "x = layers.Flatten()(base_model.output)\n",
    "x = layers.Dense(1024, activation='relu')(x)\n",
    "# x = layers.Dropout(0.2)(x)\n",
    "\n",
    "# Add a final sigmoid layer with 1 node for classification output\n",
    "x = layers.Dense(3 , activation='softmax')(x)\n",
    "\n",
    "model = tf.keras.models.Model(base_model.input, x)\n",
    "\n",
    "model.compile(\n",
    "    optimizer='adam',\n",
    "    loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=False),\n",
    "    metrics=['accuracy']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "2f328068",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\priyanshu\\AppData\\Local\\Temp\\ipykernel_12272\\3511032646.py:1: UserWarning: `Model.fit_generator` is deprecated and will be removed in a future version. Please use `Model.fit`, which supports generators.\n",
      "  inc_history = model.fit_generator(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "76/76 [==============================] - 16s 160ms/step - loss: 5.6605 - accuracy: 0.5308 - val_loss: 0.8334 - val_accuracy: 0.6642\n",
      "Epoch 2/30\n",
      "76/76 [==============================] - 10s 129ms/step - loss: 0.7840 - accuracy: 0.6628 - val_loss: 0.7330 - val_accuracy: 0.6940\n",
      "Epoch 3/30\n",
      "76/76 [==============================] - 9s 124ms/step - loss: 0.7364 - accuracy: 0.6811 - val_loss: 0.6946 - val_accuracy: 0.6866\n",
      "Epoch 4/30\n",
      "76/76 [==============================] - 10s 127ms/step - loss: 0.7268 - accuracy: 0.6894 - val_loss: 0.7351 - val_accuracy: 0.6642\n",
      "Epoch 5/30\n",
      "76/76 [==============================] - 10s 126ms/step - loss: 0.7148 - accuracy: 0.7073 - val_loss: 0.6272 - val_accuracy: 0.7500\n",
      "Epoch 6/30\n",
      "76/76 [==============================] - 10s 129ms/step - loss: 0.6789 - accuracy: 0.7252 - val_loss: 0.7088 - val_accuracy: 0.7015\n",
      "Epoch 7/30\n",
      "76/76 [==============================] - 12s 155ms/step - loss: 0.6741 - accuracy: 0.7206 - val_loss: 0.6541 - val_accuracy: 0.7351\n",
      "Epoch 8/30\n",
      "76/76 [==============================] - 10s 128ms/step - loss: 0.6516 - accuracy: 0.7361 - val_loss: 0.6613 - val_accuracy: 0.7201\n",
      "Epoch 9/30\n",
      "76/76 [==============================] - 10s 126ms/step - loss: 0.6592 - accuracy: 0.7261 - val_loss: 0.5849 - val_accuracy: 0.7575\n",
      "Epoch 10/30\n",
      "76/76 [==============================] - 11s 138ms/step - loss: 0.6408 - accuracy: 0.7473 - val_loss: 0.5772 - val_accuracy: 0.7537\n",
      "Epoch 11/30\n",
      "76/76 [==============================] - 9s 123ms/step - loss: 0.6290 - accuracy: 0.7415 - val_loss: 0.6253 - val_accuracy: 0.7425\n",
      "Epoch 12/30\n",
      "76/76 [==============================] - 10s 128ms/step - loss: 0.6074 - accuracy: 0.7540 - val_loss: 0.5713 - val_accuracy: 0.7724\n",
      "Epoch 13/30\n",
      "76/76 [==============================] - 9s 123ms/step - loss: 0.6229 - accuracy: 0.7319 - val_loss: 0.6403 - val_accuracy: 0.7276\n",
      "Epoch 14/30\n",
      "76/76 [==============================] - 10s 132ms/step - loss: 0.5832 - accuracy: 0.7444 - val_loss: 0.5262 - val_accuracy: 0.7836\n",
      "Epoch 15/30\n",
      "76/76 [==============================] - 10s 125ms/step - loss: 0.5884 - accuracy: 0.7606 - val_loss: 0.5553 - val_accuracy: 0.7761\n",
      "Epoch 16/30\n",
      "76/76 [==============================] - 10s 128ms/step - loss: 0.6115 - accuracy: 0.7423 - val_loss: 0.5686 - val_accuracy: 0.7687\n",
      "Epoch 17/30\n",
      "76/76 [==============================] - 9s 123ms/step - loss: 0.5868 - accuracy: 0.7481 - val_loss: 0.5717 - val_accuracy: 0.7649\n",
      "Epoch 18/30\n",
      "76/76 [==============================] - 10s 130ms/step - loss: 0.5675 - accuracy: 0.7631 - val_loss: 0.5461 - val_accuracy: 0.7799\n",
      "Epoch 19/30\n",
      "76/76 [==============================] - 9s 124ms/step - loss: 0.6068 - accuracy: 0.7527 - val_loss: 0.5735 - val_accuracy: 0.7575\n",
      "Epoch 20/30\n",
      "76/76 [==============================] - 10s 128ms/step - loss: 0.5798 - accuracy: 0.7627 - val_loss: 0.5761 - val_accuracy: 0.7537\n",
      "Epoch 21/30\n",
      "76/76 [==============================] - 9s 124ms/step - loss: 0.5720 - accuracy: 0.7748 - val_loss: 0.5491 - val_accuracy: 0.7687\n",
      "Epoch 22/30\n",
      "76/76 [==============================] - 10s 130ms/step - loss: 0.5630 - accuracy: 0.7644 - val_loss: 0.5520 - val_accuracy: 0.7948\n",
      "Epoch 23/30\n",
      "76/76 [==============================] - 9s 124ms/step - loss: 0.5713 - accuracy: 0.7681 - val_loss: 0.5541 - val_accuracy: 0.7799\n",
      "Epoch 24/30\n",
      "76/76 [==============================] - 10s 129ms/step - loss: 0.5852 - accuracy: 0.7631 - val_loss: 0.5537 - val_accuracy: 0.7537\n",
      "Epoch 25/30\n",
      "76/76 [==============================] - 9s 123ms/step - loss: 0.5657 - accuracy: 0.7619 - val_loss: 0.5348 - val_accuracy: 0.7761\n",
      "Epoch 26/30\n",
      "76/76 [==============================] - 10s 130ms/step - loss: 0.5782 - accuracy: 0.7635 - val_loss: 0.5437 - val_accuracy: 0.7985\n",
      "Epoch 27/30\n",
      "76/76 [==============================] - 9s 123ms/step - loss: 0.5564 - accuracy: 0.7689 - val_loss: 0.5426 - val_accuracy: 0.7799\n",
      "Epoch 28/30\n",
      "76/76 [==============================] - 10s 128ms/step - loss: 0.5482 - accuracy: 0.7773 - val_loss: 0.5458 - val_accuracy: 0.7836\n",
      "Epoch 29/30\n",
      "76/76 [==============================] - 9s 123ms/step - loss: 0.5417 - accuracy: 0.7839 - val_loss: 0.5137 - val_accuracy: 0.7836\n",
      "Epoch 30/30\n",
      "76/76 [==============================] - 10s 130ms/step - loss: 0.5528 - accuracy: 0.7698 - val_loss: 0.5546 - val_accuracy: 0.7649\n"
     ]
    }
   ],
   "source": [
    "inc_history = model.fit(\n",
    "    train_generator_inception, \n",
    "    validation_data = validation_generator_inception, \n",
    "    steps_per_epoch = 76, \n",
    "    epochs = 30\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddfc5dee",
   "metadata": {},
   "source": [
    "# resnet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "2900dce1",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from tensorflow.keras.applications import ResNet50\n",
    "\n",
    "base_model_resnet = ResNet50(input_shape=(224, 224,3), include_top=False, weights=\"imagenet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "9e920f38",
   "metadata": {},
   "outputs": [],
   "source": [
    "for layer in base_model_resnet.layers:\n",
    "    layer.trainable = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "b271a647",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.applications import ResNet50\n",
    "from tensorflow.python.keras.models import Sequential\n",
    "from tensorflow.python.keras.layers import Dense, Flatten, GlobalAveragePooling2D\n",
    "\n",
    "base_model_resnet = Sequential()\n",
    "base_model_resnet.add(ResNet50(include_top=False, weights='imagenet', pooling='max'))\n",
    "base_model_resnet.add(Dense(3, activation='softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "0bf2a0dd",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "You must compile your model before training/testing. Use `model.compile(optimizer, loss)`.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Input \u001b[1;32mIn [53]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[0m resnet_history \u001b[38;5;241m=\u001b[39m \u001b[43mbase_model_resnet\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m      2\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrain_generator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[0;32m      3\u001b[0m \u001b[43m    \u001b[49m\u001b[43mvalidation_data\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mvalidation_generator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[0;32m      4\u001b[0m \u001b[43m    \u001b[49m\u001b[43msteps_per_epoch\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m76\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[0;32m      5\u001b[0m \u001b[43m    \u001b[49m\u001b[43mepochs\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m30\u001b[39;49m\n\u001b[0;32m      6\u001b[0m \u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\tensorflow\\python\\keras\\engine\\training.py:1111\u001b[0m, in \u001b[0;36mModel.fit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[0;32m   1109\u001b[0m \u001b[38;5;66;03m# Legacy graph support is contained in `training_v1.Model`.\u001b[39;00m\n\u001b[0;32m   1110\u001b[0m version_utils\u001b[38;5;241m.\u001b[39mdisallow_legacy_graph(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mModel\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfit\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m-> 1111\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_assert_compile_was_called\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1112\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_call_args(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfit\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m   1113\u001b[0m _disallow_inside_tf_function(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfit\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\tensorflow\\python\\keras\\engine\\training.py:2725\u001b[0m, in \u001b[0;36mModel._assert_compile_was_called\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   2719\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_assert_compile_was_called\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m   2720\u001b[0m   \u001b[38;5;66;03m# Checks whether `compile` has been called. If it has been called,\u001b[39;00m\n\u001b[0;32m   2721\u001b[0m   \u001b[38;5;66;03m# then the optimizer is set. This is different from whether the\u001b[39;00m\n\u001b[0;32m   2722\u001b[0m   \u001b[38;5;66;03m# model is compiled\u001b[39;00m\n\u001b[0;32m   2723\u001b[0m   \u001b[38;5;66;03m# (i.e. whether the model is built and its inputs/outputs are set).\u001b[39;00m\n\u001b[0;32m   2724\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_is_compiled:\n\u001b[1;32m-> 2725\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mYou must compile your model before \u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m   2726\u001b[0m                        \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtraining/testing. \u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m   2727\u001b[0m                        \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mUse `model.compile(optimizer, loss)`.\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[1;31mRuntimeError\u001b[0m: You must compile your model before training/testing. Use `model.compile(optimizer, loss)`."
     ]
    }
   ],
   "source": [
    "resnet_history = base_model_resnet.fit(\n",
    "    train_generator, \n",
    "    validation_data = validation_generator, \n",
    "    steps_per_epoch = 76, \n",
    "    epochs = 30\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2743683",
   "metadata": {},
   "source": [
    "# EfficientNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "d210148e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Collecting efficientnet\n",
      "  Downloading efficientnet-1.1.1-py3-none-any.whl (18 kB)\n",
      "Collecting keras-applications<=1.0.8,>=1.0.7\n",
      "  Downloading Keras_Applications-1.0.8-py3-none-any.whl (50 kB)\n",
      "     ---------------------------------------- 50.7/50.7 kB 2.5 MB/s eta 0:00:00\n",
      "Requirement already satisfied: scikit-image in c:\\users\\priyanshu\\appdata\\roaming\\python\\python310\\site-packages (from efficientnet) (0.19.3)\n",
      "Requirement already satisfied: numpy>=1.9.1 in c:\\users\\priyanshu\\appdata\\roaming\\python\\python310\\site-packages (from keras-applications<=1.0.8,>=1.0.7->efficientnet) (1.23.0)\n",
      "Requirement already satisfied: h5py in c:\\users\\priyanshu\\appdata\\roaming\\python\\python310\\site-packages (from keras-applications<=1.0.8,>=1.0.7->efficientnet) (3.7.0)\n",
      "Requirement already satisfied: tifffile>=2019.7.26 in c:\\users\\priyanshu\\appdata\\roaming\\python\\python310\\site-packages (from scikit-image->efficientnet) (2022.5.4)\n",
      "Requirement already satisfied: networkx>=2.2 in c:\\users\\priyanshu\\appdata\\roaming\\python\\python310\\site-packages (from scikit-image->efficientnet) (2.8.4)\n",
      "Requirement already satisfied: PyWavelets>=1.1.1 in c:\\users\\priyanshu\\appdata\\roaming\\python\\python310\\site-packages (from scikit-image->efficientnet) (1.3.0)\n",
      "Requirement already satisfied: imageio>=2.4.1 in c:\\users\\priyanshu\\appdata\\roaming\\python\\python310\\site-packages (from scikit-image->efficientnet) (2.19.3)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\priyanshu\\appdata\\roaming\\python\\python310\\site-packages (from scikit-image->efficientnet) (21.3)\n",
      "Requirement already satisfied: scipy>=1.4.1 in c:\\users\\priyanshu\\appdata\\roaming\\python\\python310\\site-packages (from scikit-image->efficientnet) (1.8.1)\n",
      "Requirement already satisfied: pillow!=7.1.0,!=7.1.1,!=8.3.0,>=6.1.0 in c:\\users\\priyanshu\\appdata\\roaming\\python\\python310\\site-packages (from scikit-image->efficientnet) (9.1.1)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in c:\\users\\priyanshu\\appdata\\roaming\\python\\python310\\site-packages (from packaging>=20.0->scikit-image->efficientnet) (3.0.9)\n",
      "Installing collected packages: keras-applications, efficientnet\n",
      "Successfully installed efficientnet-1.1.1 keras-applications-1.0.8\n",
      "\n",
      "[notice] A new release of pip available: 22.1.2 -> 22.2.2\n",
      "[notice] To update, run: C:\\Program Files\\Python310\\python.exe -m pip install --upgrade pip\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install efficientnet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "2a530146",
   "metadata": {},
   "outputs": [],
   "source": [
    "import efficientnet.keras as efn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "f2b43c07",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_model_efficient = efn.EfficientNetB0(input_shape = (224, 224, 3), include_top = False, weights = 'imagenet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "1feb07f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "for layer in base_model_efficient.layers:\n",
    "    layer.trainable = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "983a6bfe",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Attempt to convert a value (None) with an unsupported type (<class 'NoneType'>) to a Tensor.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Input \u001b[1;32mIn [70]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[0m x1 \u001b[38;5;241m=\u001b[39m \u001b[43mFlatten\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbase_model_efficient\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moutput\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      3\u001b[0m x1 \u001b[38;5;241m=\u001b[39m Dense(\u001b[38;5;241m1024\u001b[39m, activation\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrelu\u001b[39m\u001b[38;5;124m\"\u001b[39m)(x1)\n\u001b[0;32m      4\u001b[0m \u001b[38;5;66;03m# x = Dropout(0.5)(x)\u001b[39;00m\n\u001b[0;32m      5\u001b[0m \n\u001b[0;32m      6\u001b[0m \u001b[38;5;66;03m# Add a final sigmoid layer with 1 node for classification output\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\tensorflow\\python\\keras\\engine\\base_layer.py:1044\u001b[0m, in \u001b[0;36mLayer.__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1040\u001b[0m   inputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_maybe_cast_inputs(inputs, input_list)\n\u001b[0;32m   1042\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m autocast_variable\u001b[38;5;241m.\u001b[39menable_auto_cast_variables(\n\u001b[0;32m   1043\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compute_dtype_object):\n\u001b[1;32m-> 1044\u001b[0m   outputs \u001b[38;5;241m=\u001b[39m call_fn(inputs, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1046\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_activity_regularizer:\n\u001b[0;32m   1047\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_handle_activity_regularization(inputs, outputs)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\tensorflow\\python\\keras\\layers\\core.py:666\u001b[0m, in \u001b[0;36mFlatten.call\u001b[1;34m(self, inputs)\u001b[0m\n\u001b[0;32m    661\u001b[0m     inputs \u001b[38;5;241m=\u001b[39m array_ops\u001b[38;5;241m.\u001b[39mtranspose(inputs, perm\u001b[38;5;241m=\u001b[39mpermutation)\n\u001b[0;32m    663\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m context\u001b[38;5;241m.\u001b[39mexecuting_eagerly():\n\u001b[0;32m    664\u001b[0m   \u001b[38;5;66;03m# Full static shape is guaranteed to be available.\u001b[39;00m\n\u001b[0;32m    665\u001b[0m   \u001b[38;5;66;03m# Performance: Using `constant_op` is much faster than passing a list.\u001b[39;00m\n\u001b[1;32m--> 666\u001b[0m   flattened_shape \u001b[38;5;241m=\u001b[39m \u001b[43mconstant_op\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconstant\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshape\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    667\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m array_ops\u001b[38;5;241m.\u001b[39mreshape(inputs, flattened_shape)\n\u001b[0;32m    668\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\tensorflow\\python\\framework\\constant_op.py:267\u001b[0m, in \u001b[0;36mconstant\u001b[1;34m(value, dtype, shape, name)\u001b[0m\n\u001b[0;32m    170\u001b[0m \u001b[38;5;129m@tf_export\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mconstant\u001b[39m\u001b[38;5;124m\"\u001b[39m, v1\u001b[38;5;241m=\u001b[39m[])\n\u001b[0;32m    171\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mconstant\u001b[39m(value, dtype\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, shape\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mConst\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m    172\u001b[0m   \u001b[38;5;124;03m\"\"\"Creates a constant tensor from a tensor-like object.\u001b[39;00m\n\u001b[0;32m    173\u001b[0m \n\u001b[0;32m    174\u001b[0m \u001b[38;5;124;03m  Note: All eager `tf.Tensor` values are immutable (in contrast to\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    265\u001b[0m \u001b[38;5;124;03m    ValueError: if called on a symbolic tensor.\u001b[39;00m\n\u001b[0;32m    266\u001b[0m \u001b[38;5;124;03m  \"\"\"\u001b[39;00m\n\u001b[1;32m--> 267\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_constant_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mshape\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverify_shape\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    268\u001b[0m \u001b[43m                        \u001b[49m\u001b[43mallow_broadcast\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\tensorflow\\python\\framework\\constant_op.py:279\u001b[0m, in \u001b[0;36m_constant_impl\u001b[1;34m(value, dtype, shape, name, verify_shape, allow_broadcast)\u001b[0m\n\u001b[0;32m    277\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m trace\u001b[38;5;241m.\u001b[39mTrace(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtf.constant\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m    278\u001b[0m       \u001b[38;5;28;01mreturn\u001b[39;00m _constant_eager_impl(ctx, value, dtype, shape, verify_shape)\n\u001b[1;32m--> 279\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_constant_eager_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43mctx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mshape\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverify_shape\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    281\u001b[0m g \u001b[38;5;241m=\u001b[39m ops\u001b[38;5;241m.\u001b[39mget_default_graph()\n\u001b[0;32m    282\u001b[0m tensor_value \u001b[38;5;241m=\u001b[39m attr_value_pb2\u001b[38;5;241m.\u001b[39mAttrValue()\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\tensorflow\\python\\framework\\constant_op.py:304\u001b[0m, in \u001b[0;36m_constant_eager_impl\u001b[1;34m(ctx, value, dtype, shape, verify_shape)\u001b[0m\n\u001b[0;32m    302\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_constant_eager_impl\u001b[39m(ctx, value, dtype, shape, verify_shape):\n\u001b[0;32m    303\u001b[0m   \u001b[38;5;124;03m\"\"\"Creates a constant on the current device.\"\"\"\u001b[39;00m\n\u001b[1;32m--> 304\u001b[0m   t \u001b[38;5;241m=\u001b[39m \u001b[43mconvert_to_eager_tensor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mctx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    305\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m shape \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    306\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m t\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\tensorflow\\python\\framework\\constant_op.py:102\u001b[0m, in \u001b[0;36mconvert_to_eager_tensor\u001b[1;34m(value, ctx, dtype)\u001b[0m\n\u001b[0;32m    100\u001b[0m     dtype \u001b[38;5;241m=\u001b[39m dtypes\u001b[38;5;241m.\u001b[39mas_dtype(dtype)\u001b[38;5;241m.\u001b[39mas_datatype_enum\n\u001b[0;32m    101\u001b[0m ctx\u001b[38;5;241m.\u001b[39mensure_initialized()\n\u001b[1;32m--> 102\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mops\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mEagerTensor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mctx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdevice_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mValueError\u001b[0m: Attempt to convert a value (None) with an unsupported type (<class 'NoneType'>) to a Tensor."
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "x1 = Flatten()(base_model_efficient.output)\n",
    "\n",
    "x1 = Dense(1024, activation=\"relu\")(x1)\n",
    "# x = Dropout(0.5)(x)\n",
    "\n",
    "# Add a final sigmoid layer with 1 node for classification output\n",
    "predictions = Dense(3, activation=\"softmax\")(x1)\n",
    "\n",
    "model_final = Model(input = base_model_efficient.input, output = predictions)\n",
    "model_final.compile(optimizers.rmsprop(lr=0.0001, decay=1e-6),loss='binary_crossentropy',metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77432f26",
   "metadata": {},
   "outputs": [],
   "source": [
    "eff_history = model_final.fit_generator(\n",
    "    train_generator, \n",
    "    validation_data = validation_generator, \n",
    "    steps_per_epoch = 76, \n",
    "    epochs = 30\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b879b2e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
